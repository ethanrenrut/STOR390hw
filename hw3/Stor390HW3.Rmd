---
title: "HW 3"
author: "Ethan Turner"
date: "02/28/2024"
output:
  html_document:
    number_sections: yes
  word_document: default
---

# 

In this homework, we will discuss support vector machines and tree-based methods.  I will begin by simulating some data for you to use with SVM. 

```{r}
library(e1071)
set.seed(1) 
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x, col=y)


```


##

Quite clearly, the above data is not linearly separable.  Create a training-testing partition with 100 random observations in the training partition.  Fit an svm on this training data using the radial kernel, and tuning parameters $\gamma=1$, cost $=1$.  Plot the svm on the training data.  

```{r}
train <- sample(1:nrow(x), 0.5 * nrow(x))

xtrain <- x[train,]
ytrain <- y[train]

xtest <- x[-train,]
ytest <- y[-train]

library(e1071)

dat = data.frame(x, y = as.factor(y))




svmfit = svm(y~., data = dat[train,], kernel = "radial", cost = 1, gamma = 1, scale = FALSE)



```


```{r}
make.grid = function(xtrain, n = 75){ #n is optional argument
  grange = apply(xtrain, 2, range) #range is built-in fn
  x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
  x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
  expand.grid(X1 = x1, X2 = x2)
}


xgrid = make.grid(x)

ygrid = predict(svmfit, xgrid)

plot(xgrid, col = c('black', 'red')[as.numeric(ygrid)], pch = 20, cex = 0.2)

points(xtrain, col = ytrain, pch = 19)
points(xtrain[svmfit$index,], pch = 5, cex = 2)
```

##

Notice that the above decision boundary is decidedly non-linear.  It seems to perform reasonably well, but there are indeed some misclassifications.  Let's see if increasing the cost ^[Remember this is a parameter that decides how smooth your decision boundary should be] helps our classification error rate.  Refit the svm with the radial kernel, $\gamma=1$, and a cost of 10000.  Plot this svm on the training data. 

```{r}
svmfit = svm(y~., data = dat[train,], kernel = "radial", cost = 10000, gamma = 1, scale = FALSE)

plot(xgrid, col = c('black', 'red')[as.numeric(ygrid)], pch = 20, cex = 0.2)

points(xtrain, col = ytrain, pch = 19)
points(xtrain[svmfit$index,], pch = 5, cex = 2)
```

##

It would appear that we are better capturing the training data, but comment on the dangers (if any exist), of such a model. 

Overfitting might be an issue, because for a very high error cost then we might simply be training the model to replicate the training set rather than make applicable conclusions. Also, with a radial kernel it seems difficult to separate the numerous close-together conflicting classifications evident here.

##

Create a confusion matrix by using this svm to predict on the current testing partition.  Comment on the confusion matrix.  Is there any disparity in our classification results?    

```{r}

table(true=dat[-train,"y"], pred=predict(svmfit, newdata=dat[-train,]))
```


There is quite a disparity, with 22 misses out of 100 far less accurate than for the training set. 

##

Is this disparity because of imbalance in the training/testing partition?  Find the proportion of class `2` in your training partition and see if it is broadly representative of the underlying 25\% of class 2 in the data as a whole.  

```{r}
sum(dat[train,]$y == 2)
```

27% of the training set is class 2, which is slightly above the 25% of data in class 2 on the whole.

##

Let's try and balance the above to solutions via cross-validation.  Using the `tune` function, pass in the training data, and a list of the following cost and $\gamma$ values: {0.1, 1, 10, 100, 1000} and {0.5, 1,2,3,4}.  Save the output of this function in a variable called `tune.out`.  

```{r}
set.seed(1)

tune.out = tune(svm, y~., data = dat[train,], ranges = list(cost = 10^(-1:3), gamma = c(.5, 1, 2, 3, 4)))

```

I will take `tune.out` and use the best model according to error rate to test on our data.  I will report a confusion matrix corresponding to the 100 predictions.  


```{r}
table(true=dat[-train,"y"], pred=predict(tune.out$best.model, newdata=dat[-train,]))

```

##

Comment on the confusion matrix.  How have we improved upon the model in question 2 and what qualifications are still necessary for this improved model.  

The amount of missed classifications has shrunk. We still might want to experiment with other kernels to see if our results improve further, since we have still misclassified 12% of the testing data. Optimally, we might introduce new data to extinguish any last doubts about overfitting. 

# 
Let's turn now to decision trees.  

```{r}

library(kmed)
data(heart)
library(tree)

```

## 

The response variable is currently a categorical variable with four levels.  Convert heart disease into binary categorical variable.  Then, ensure that it is properly stored as a factor. 

```{r}
disease <- ifelse(heart$class > 0, 1, 0)

disease <- as.factor(disease)

heart = data.frame(heart, disease)

heart = heart[-14]


```

## 

Train a classification tree on a 240 observation training subset (using the seed I have set for you).  Plot the tree.  

```{r}
set.seed(101)

train <- sample(1:nrow(heart), (240/nrow(heart)) * nrow(heart))

library(rpart.plot)



heart.tree <- rpart(disease~. -disease, data = heart[train,], method = "class")

par(xpd = NA)

plot(heart.tree)
text(heart.tree, pretty = 0)


rpart.plot(heart.tree)

```


## 

Use the trained model to classify the remaining testing points.  Create a confusion matrix to evaluate performance.  Report the classification error rate.  

```{r}
tree.pred = predict(heart.tree, heart[-train,], type="class")
with(heart[-train,], table(tree.pred, disease))

1-((29+16)/(29+5+7+16))

```

##  

Above we have a fully grown (bushy) tree.  Now, cross validate it using the `cv.tree` command.  Specify cross validation to be done according to the misclassification rate.  Choose an ideal number of splits, and plot this tree.  Finally, use this pruned tree to test on the testing set.  Report a confusion matrix and the misclassification rate.  

```{r}
tree.heart = tree(disease~. -disease, heart, subset = train)


cv.heart <- cv.tree(tree.heart, FUN = prune.misclass)
cv.heart

plot(cv.heart$size, cv.heart$dev, type = "b")

prune.heart = prune.misclass(tree.heart, best = 4)


plot(prune.heart)
text(prune.heart, pretty=0)

tree.pred = predict(prune.heart, heart[-train,], type="class")
with(heart[-train,], table(tree.pred, disease))

1-(26+17)/(26+17+10+4)

```


##

Discuss the trade-off in accuracy and interpretability in pruning the above tree. 

We have lost slightly over 3% in terms of accuracy, but the tree is far simpler and easier to interpret. This seems to be an advantageous tradeoff.

## 

Discuss the ways a decision tree could manifest algorithmic bias.  

Good training data is crucial to the application of a decision tree. If training data is poorly balanced in some way, then a tree ignores certain key explanatory variables for categories largely neglected by the selection of training data. If we have any case where data is ordered in relation to a specific variable, like the Titanic example discussed in class where we did not shuffle the data to create our training set, then we will encounter both reduced model accuracy and reduced universal applicability. Away from technology, simply neglecting to sample certain groups when physically collecting data also harms the universality of a decision tree.
